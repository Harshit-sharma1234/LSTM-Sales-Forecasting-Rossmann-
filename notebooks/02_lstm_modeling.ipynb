{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Rossmann Sales â€” LSTM Modeling\n",
        "\n",
        "This notebook builds an LSTM model for sales forecasting using the processed dataset saved from the EDA notebook. It creates lag/rolling features, windowed sequences, performs time-based splits, trains with early stopping, evaluates, plots results, and saves artifacts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "RAW_DIR = Path('data/raw')\n",
        "PROC_DIR = Path('data/processed')\n",
        "MODEL_DIR = Path('models'); MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "REPORT_DIR = Path('reports/figures'); REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "proc_path = PROC_DIR / 'rossmann_processed.csv'\n",
        "assert proc_path.exists(), f\"Processed dataset not found at {proc_path}. Run 01_data_loading_eda.ipynb first.\"\n",
        "\n",
        "df = pd.read_csv(proc_path, parse_dates=['Date'])\n",
        "df = df.sort_values(['Store','Date']).reset_index(drop=True)\n",
        "print(df.shape)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature engineering: create lags and rolling means per store\n",
        "\n",
        "def add_lags_and_rolls(frame: pd.DataFrame, target_col: str, lags=(1,7,14,28), roll_windows=(7,28)) -> pd.DataFrame:\n",
        "    g = frame.copy()\n",
        "    for L in lags:\n",
        "        g[f\"lag_{L}\"] = g.groupby('Store')[target_col].shift(L)\n",
        "    for w in roll_windows:\n",
        "        g[f\"roll{w}\"] = g.groupby('Store')[target_col].shift(1).rolling(w).mean()\n",
        "    return g\n",
        "\n",
        "# Apply on log target\n",
        "feat_df = add_lags_and_rolls(df, 'Sales_log')\n",
        "feat_df = feat_df.dropna().reset_index(drop=True)\n",
        "\n",
        "feature_cols = ['dow','month','year','week','Open','Promo','SchoolHoliday'] + \\\n",
        "               [f'lag_{L}' for L in (1,7,14,28)] + [f'roll{w}' for w in (7,28)]\n",
        "\n",
        "y = feat_df['Sales_log'].values.astype('float32')\n",
        "X_num = feat_df[feature_cols].values.astype('float32')\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_num_scaled = scaler.fit_transform(X_num)\n",
        "X_num_scaled[:2], y[:2]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sequences per store (lookback -> next day)\n",
        "from typing import Tuple\n",
        "\n",
        "def make_sequences_by_store(frame: pd.DataFrame, X_scaled: np.ndarray, target: np.ndarray, feature_cols, lookback: int = 28, horizon: int = 1) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    X_list, y_list = [], []\n",
        "    start_idx = 0\n",
        "    for store_id, g in frame.groupby('Store'):\n",
        "        g = g.sort_values('Date')\n",
        "        n = len(g)\n",
        "        Xg = X_scaled[start_idx:start_idx+n]\n",
        "        yg = target[start_idx:start_idx+n]\n",
        "        for i in range(lookback, n - horizon + 1):\n",
        "            X_list.append(Xg[i - lookback:i, :])\n",
        "            y_list.append(yg[i:i + horizon])\n",
        "        start_idx += n\n",
        "    return np.array(X_list, dtype=np.float32), np.array(y_list, dtype=np.float32)\n",
        "\n",
        "lookback, horizon = 28, 1\n",
        "X_seq, y_seq = make_sequences_by_store(feat_df, X_num_scaled, y, feature_cols, lookback, horizon)\n",
        "X_seq.shape, y_seq.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Time-based split: use the last N sequences as test, and a validation split from the remainder\n",
        "n = len(X_seq)\n",
        "print('Total sequences:', n)\n",
        "\n",
        "test_size = int(0.1 * n)\n",
        "val_size = int(0.1 * (n - test_size))\n",
        "\n",
        "X_train_full, y_train_full = X_seq[: n - test_size], y_seq[: n - test_size]\n",
        "X_test, y_test = X_seq[n - test_size :], y_seq[n - test_size :]\n",
        "\n",
        "X_train, y_train = X_train_full[: n - test_size - val_size], y_train_full[: n - test_size - val_size]\n",
        "X_val, y_val = X_train_full[n - test_size - val_size :], y_train_full[n - test_size - val_size :]\n",
        "\n",
        "list(map(lambda a: a.shape, [X_train, y_train, X_val, y_val, X_test, y_test]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build the LSTM model\n",
        "n_features = X_seq.shape[-1]\n",
        "\n",
        "def build_model(units=64, dropout=0.2, lr=1e-3, lookback=28, n_features=None, horizon=1):\n",
        "    inputs = keras.Input(shape=(lookback, n_features))\n",
        "    x = layers.LSTM(units, return_sequences=False)(inputs)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    outputs = layers.Dense(horizon)(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "model = build_model(64, 0.2, 1e-3, lookback, n_features, horizon)\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train with early stopping\n",
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping(monitor='val_mae', patience=5, restore_best_weights=True)\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=50,\n",
        "    batch_size=64,\n",
        "    verbose=1,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# Plot training curves\n",
        "plt.figure(figsize=(10,3))\n",
        "plt.plot(history.history['mae'], label='train mae')\n",
        "plt.plot(history.history['val_mae'], label='val mae')\n",
        "plt.legend(); plt.title('Training curves (MAE)'); plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test\n",
        "pred_test = model.predict(X_test, verbose=0).squeeze()\n",
        "\n",
        "y_true = np.expm1(y_test.squeeze())\n",
        "y_pred = np.expm1(pred_test)\n",
        "\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
        "print({'test_mae': mae, 'test_rmse': rmse})\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(y_true[:500], label='Actual')\n",
        "plt.plot(y_pred[:500], label='Predicted')\n",
        "plt.legend(); plt.title('Test set (first 500)'); plt.tight_layout()\n",
        "plot_path = REPORT_DIR / 'lstm_test_plot.png'\n",
        "plt.savefig(plot_path, dpi=150)\n",
        "print('Saved plot to', plot_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save artifacts\n",
        "model_path = MODEL_DIR / 'rossmann_lstm.keras'\n",
        "model.save(model_path)\n",
        "print('Saved model to', model_path)\n",
        "\n",
        "# Save scaler for inference\n",
        "import joblib\n",
        "scaler_path = MODEL_DIR / 'feature_scaler.pkl'\n",
        "joblib.dump(scaler, scaler_path)\n",
        "print('Saved scaler to', scaler_path)\n",
        "\n",
        "# Save predictions\n",
        "preds_path = MODEL_DIR / 'test_predictions.csv'\n",
        "pd.DataFrame({'y_true': y_true, 'y_pred': y_pred}).to_csv(preds_path, index=False)\n",
        "print('Saved predictions to', preds_path)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
